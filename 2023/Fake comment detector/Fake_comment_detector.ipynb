{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "필요한 라이브러리 및 모듈 임포트"
      ],
      "metadata": {
        "id": "p7bBhbv0eblC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AG9up7B6lqM0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd.variable import Variable\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator 클래스 정의"
      ],
      "metadata": {
        "id": "ez1N3Fv8ee7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Generoator 역할: 실제 data를 기반으로 실제와 비슷한 data 생성\n",
        "    input (Tensor): 실제 data\n",
        "    output (Tensor): generator로 생성한 data\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained_model='gpt2'):\n",
        "        super(Generator, self).__init__()\n",
        "        self.generator = GPT2LMHeadModel.from_pretrained(pretrained_model)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.generator(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "TZkRNdcSmVgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator 클래스 정의"
      ],
      "metadata": {
        "id": "Lmqm-1UfgH_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminator 역할: data가 real 인지 fake인지 예측\n",
        "    input (Tensor): real data or fake data\n",
        "    output (Tensor): real or fake\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "vn162AmknmmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gan 클래스 정의"
      ],
      "metadata": {
        "id": "soewRJeUhz5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(nn.Module):\n",
        "    \"\"\"\n",
        "    Generator + Discriminator\n",
        "\n",
        "    input (Tensor): real data or fake data\n",
        "    output (Tensor): real or fake\n",
        "    \"\"\"\n",
        "    def __init__(self, generator, discriminator):\n",
        "        super(GAN, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def forward(self, x):\n",
        "        generated_data = self.generator(x)\n",
        "        output = self.discriminator(generated_data.float())\n",
        "        return output"
      ],
      "metadata": {
        "id": "ah6mW0Y5GjPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "real & fake data 생성"
      ],
      "metadata": {
        "id": "ppEszttXgLEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_real_and_fake_data(batch_data, generator, tokenizer, max_length=50):\n",
        "    \"\"\"\n",
        "    실제 문장이 들어오면 모델에서 사용할 real data 형태로 바꾸고, fake data를 생성시키는 함수\n",
        "\n",
        "    input:\n",
        "    - batch_data (list): 배치로 구성된 실제 문장 리스트\n",
        "    - generator (Generator): 가짜 데이터 생성을 담당하는 생성자 모델\n",
        "    - tokenizer (GPT2Tokenizer): 토크나이저\n",
        "    - max_length (int): 토큰의 최대 길이 (기본값: 50)\n",
        "\n",
        "    반환:\n",
        "    - real_indexes (Tensor): real_data의 index로 구성된 텐서 size = (batch, max_length)\n",
        "    - fake_indexes (Tensor): 생성된 fake_data의 index로 구성된 텐서 size = (batch, max_length)\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Add padding token to the tokenizer\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    real_indexes, fake_indexes = [], []\n",
        "\n",
        "    for real_data in batch_data:\n",
        "        # Process real data\n",
        "        input_ids = tokenizer.encode(real_data, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        # Real data\n",
        "        real_input_idx = pad_sequence(input_ids[0], max_length, tokenizer.pad_token_id)\n",
        "        real_indexes.append(real_input_idx)\n",
        "\n",
        "        # Generate fake data\n",
        "        output_ids = generator(input_ids).logits[0].argmax(dim=-1)\n",
        "\n",
        "        # Fake data\n",
        "        fake_output_idx = pad_sequence(output_ids, max_length, tokenizer.pad_token_id)\n",
        "        fake_indexes.append(fake_output_idx)\n",
        "\n",
        "    real_indexes = torch.stack(real_indexes)\n",
        "    fake_indexes = torch.stack(fake_indexes)\n",
        "\n",
        "    return real_indexes, fake_indexes"
      ],
      "metadata": {
        "id": "iVhEPDX2NoG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequence(sequence, max_length, pad_token_id):\n",
        "    \"\"\"\n",
        "    주어진 sequence를 max_length로 패딩하는 함수\n",
        "\n",
        "    input:\n",
        "    - sequence (Tensor): input sequence\n",
        "    - max_length (int): 최대로 패딩할 길이\n",
        "    - pad_token_id (int): 패딩에 사용할 토큰의 인덱스\n",
        "\n",
        "    output:\n",
        "    - padded_sequence (Tensor): 패딩된 sequence\n",
        "    \"\"\"\n",
        "    padding_length = max_length - len(sequence)\n",
        "\n",
        "    if padding_length > 0:\n",
        "        padded_sequence = F.pad(sequence, (0, padding_length), value=pad_token_id)\n",
        "    else:\n",
        "        padded_sequence = sequence\n",
        "\n",
        "    return padded_sequence"
      ],
      "metadata": {
        "id": "dwBF_5rvclFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "Aj5yWBEejfco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, gan, num_epochs, batch_size, learning_rate, tokenizer, data):\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    optimizer_generator = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "    optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "\n",
        "    #===========================================\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    discriminator = discriminator.to(device)\n",
        "    gan = gan.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    real_data_loader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    len_data = len(real_data_loader)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Discriminator 학습\n",
        "        for real_data in real_data_loader:\n",
        "            real_data, fake_data = generate_real_and_fake_data(real_data, generator, tokenizer)\n",
        "\n",
        "            real_data = real_data.to(device).float()\n",
        "            fake_data = fake_data.to(device).float()\n",
        "\n",
        "            real_labels = torch.ones(batch_size, 1).to(device)\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "            # Real data loss\n",
        "            output_real = discriminator(real_data)\n",
        "            loss_real = criterion(output_real, real_labels)\n",
        "            loss_real.backward()\n",
        "\n",
        "            # Fake data loss\n",
        "            output_fake = discriminator(fake_data.detach())\n",
        "            loss_fake = criterion(output_fake, fake_labels)\n",
        "            loss_fake.backward()\n",
        "\n",
        "            # Optimize\n",
        "            optimizer_discriminator.step()\n",
        "            discriminator.zero_grad()\n",
        "\n",
        "        # Generator 학습\n",
        "        ranint = random.randint(0,len_data-1)\n",
        "        input_data = real_data[ranint].to(device).float()\n",
        "        labels = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "        gan_data = gan(input_data).float()\n",
        "        loss_generator = criterion(gan_data, labels)\n",
        "        loss_generator.backward()\n",
        "\n",
        "        optimizer_generator.step()\n",
        "        generator.zero_grad()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          torch.save(generator, '/change_your_dir/generator.pt')\n",
        "          torch.save(discriminator, '/change_your_dir/discriminator.pt')\n",
        "\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}],'\n",
        "              f'Generator Loss: {loss_generator.item():.4f}, '\n",
        "              f'Discriminator Loss: {0.5 * (loss_real + loss_fake).item():.4f}')"
      ],
      "metadata": {
        "id": "BNeUwg6QM2iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name)\n",
        "generator = Generator(pretrained_model=pretrained_model_name)\n",
        "discriminator = Discriminator(50) #length_size\n",
        "gan = GAN(generator, discriminator)\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.0002\n",
        "\n",
        "data = list(pd.read_csv('your_data').dropna(axis=0, how='any')) #list 형식의 자연어 data\n",
        "\n",
        "# GAN 학습\n",
        "train_gan(generator, discriminator, gan, num_epochs, batch_size, learning_rate, tokenizer, data)"
      ],
      "metadata": {
        "id": "8ncHXdorVNko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3CBUt9kAVWWn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}